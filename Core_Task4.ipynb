{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Core_Task4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**The Entreprenuership Network**"
      ],
      "metadata": {
        "id": "nCdveZq3m0uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Core Task 4: Create a Image to emoji project.**"
      ],
      "metadata": {
        "id": "S9_AcPSlm9dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author Name : Teena Joseph(MSC DA), Rajagiri College of Social Sciences**"
      ],
      "metadata": {
        "id": "PJp-m4Z6ndPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the required libraries**"
      ],
      "metadata": {
        "id": "g-R7uos-jMlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import NumPy as np\n",
        "import cv2\n",
        "from Keras.emotion_models import Sequential\n",
        "from kerasKeras.layers import Dense\n",
        "from Keras.layers import Dropout\n",
        "from Keras.layers import Flatten\n",
        "from Keras. layers import Conv2D\n",
        "from Keras.optimizers import Adam\n",
        "from Keras. layers import MaxPooling2D\n",
        "from Keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "jqKSNeNRjxk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the training and validation generators:**"
      ],
      "metadata": {
        "id": "j_TqkRh9j0pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'data/train'\n",
        "val_dir = 'data/test'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "#training generator for CNN\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "       train_dir,\n",
        "       target_size=(48,48),\n",
        "       batch_size=64,\n",
        "       color_mode=\"gray_framescale\",\n",
        "       class_mode='categorical')\n",
        "#validation generator for CNN\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "       val_dir,\n",
        "       target_size=(48,48),\n",
        "       batch_size=64,\n",
        "       color_mode=\"gray_framescale\",\n",
        "       class_mode='categorical')"
      ],
      "metadata": {
        "id": "ejealLY-j3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "\n",
        "Found 28709 images belonging to 7 classes.\n",
        "\n",
        "Found 7178 images belonging to 7 classes."
      ],
      "metadata": {
        "id": "QBILcJH8kG9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To display the train data\n",
        "\n",
        "for i in os.listdir(\"train/\"):\n",
        "    print(str(len(os.listdir(\"train/\"+i))) +\" \"+ i +\" images\")"
      ],
      "metadata": {
        "id": "cIMB2IP5kLTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "3995 angry images\n",
        "\n",
        "436 disgust images\n",
        "\n",
        "4097 fear images\n",
        "\n",
        "7215 happy images\n",
        "\n",
        "4965 neutral images\n",
        "\n",
        "4830 sad images\n",
        "\n",
        "3171 surprise images"
      ],
      "metadata": {
        "id": "x6TsJDgOkQzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To display the test data\n",
        "\n",
        "for i in os.listdir(\"test/\"):\n",
        "    print(str(len(os.listdir(\"test/\"+i))) +\" \"+ i +\" images\")"
      ],
      "metadata": {
        "id": "0AVDl9fXkWIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "958 angry images\n",
        "\n",
        "111 disgust images\n",
        "\n",
        "1024 fear images\n",
        "\n",
        "1774 happy images\n",
        "\n",
        "1233 neutral images\n",
        "\n",
        "1247 sad images\n",
        "\n",
        "831 surprise images"
      ],
      "metadata": {
        "id": "F9_loH7XkoCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the convolution network architecture:\n",
        "\n",
        "emotion_model = Sequential()\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))#output=(48-3+0)/1+1=46\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))#output=(46-3+0)/1+1=44\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#output=devided input by 2 it means 22,22,64\n",
        "emotion_model.add(Dropout(0.25))#reduce 25% module at a time of output\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',input_shape=(48,48,1)))#(22-3+0)/1+1=20\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#10\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))#(10-3+0)/1+1=8\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#output=4\n",
        "emotion_model.add(Dropout(0.25))#nothing change\n",
        "emotion_model.add(Flatten())#here we get multidimension output and pass as linear to the dense so that 4*4*128=2048\n",
        "emotion_model.add(Dense(1024, activation='relu'))#hddien of 1024 neurons of input \n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))#hddien of 7 neurons of input\n",
        "plot_model(emotion_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)#save model leyer as model_plot.png\n",
        "emotion_model.summary()"
      ],
      "metadata": {
        "id": "NUOhQsuOkyRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train the model\n",
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit_generator( #to fetch the model info from validation generator\n",
        "       train_generator,\n",
        "       steps_per_epoch=28709 // 64,\n",
        "       epochs=50,\n",
        "       validation_data=validation_generator,\n",
        "       validation_steps=7178 // 64)"
      ],
      "metadata": {
        "id": "cfwjVrhplkeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model weights:\n",
        "emotion_model.save_weights('model.h5')#to save the model"
      ],
      "metadata": {
        "id": "xa6ohpRXln8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To detect bounding boxes of face in the webcam and to predict the emotions we use OpenCV Haarcascade xml:\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "#emotion dictionary creation\n",
        "em_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "   ret, fram = cap.read()\n",
        "if not ret:\n",
        "       break\n",
        "#bounding box initialization  \n",
        " bounding_box = cv2.CascadeClassifier('cv2/data/haarcascade_frontalface_default.xml')\n",
        "   gray_frame = cv2.cvtColor(fram, cv2.COLOR_BGR2gray_frame)\n",
        "#to detect the multiple faces and frame them separately   \n",
        "n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
        "for (x, y, w, h) in n_faces:\n",
        "       cv2.rectangle(fram, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "       roi_frame = gray_frame[y:y + h, x:x + w]\n",
        "       crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
        "       emotion_prediction = emotion_model.predict(crop_img)\n",
        "       maxindex = int(np.argmax(emotion_prediction))\n",
        "       cv2.putText(frame, em_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "   cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
        "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "   break"
      ],
      "metadata": {
        "id": "e49q8LMml_rq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}